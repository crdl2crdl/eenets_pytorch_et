Using mnist dataset
Not using data augmentation.
Train on 60000 samples, validate on 10000 samples
Epoch 1/50
 - 54s - loss: 2.1286 - conf0_loss: 2.0356 - conf1_loss: 2.0734 - out0_loss: 2.0356 - out1_loss: 2.0734 - out2_loss: 2.3923 - conf0_acc: 0.4856 - conf1_acc: 0.2703 - out0_acc: 0.1617 - out1_acc: 0.2649 - out2_acc: 0.5572 - val_loss: 1.5381 - val_conf0_loss: 1.5058 - val_conf1_loss: 1.5198 - val_out0_loss: 1.5058 - val_out1_loss: 1.5198 - val_out2_loss: 1.6024 - val_conf0_acc: 0.8997 - val_conf1_acc: 0.8201 - val_out0_acc: 0.1803 - val_out1_acc: 0.3144 - val_out2_acc: 0.8293
accuracy:  0.8138
cost:      0.926115640218
Epoch 2/50
 - 51s - loss: 1.3693 - conf0_loss: 1.3338 - conf1_loss: 1.3559 - out0_loss: 1.3338 - out1_loss: 1.3559 - out2_loss: 1.4289 - conf0_acc: 0.8836 - conf1_acc: 0.7787 - out0_acc: 0.1544 - out1_acc: 0.3036 - out2_acc: 0.8825 - val_loss: 1.3762 - val_conf0_loss: 1.3251 - val_conf1_loss: 1.3715 - val_out0_loss: 1.3251 - val_out1_loss: 1.3715 - val_out2_loss: 1.4480 - val_conf0_acc: 0.8837 - val_conf1_acc: 0.7893 - val_out0_acc: 0.1494 - val_out1_acc: 0.3200 - val_out2_acc: 0.8588
accuracy:  0.8379
cost:      0.879347808651
Epoch 3/50
 - 50s - loss: 1.2376 - conf0_loss: 1.1961 - conf1_loss: 1.2258 - out0_loss: 1.1961 - out1_loss: 1.2258 - out2_loss: 1.3039 - conf0_acc: 0.8590 - conf1_acc: 0.8012 - out0_acc: 0.1187 - out1_acc: 0.2915 - out2_acc: 0.9151 - val_loss: 1.2060 - val_conf0_loss: 1.1560 - val_conf1_loss: 1.1947 - val_out0_loss: 1.1560 - val_out1_loss: 1.1947 - val_out2_loss: 1.2877 - val_conf0_acc: 0.8639 - val_conf1_acc: 0.8026 - val_out0_acc: 0.1181 - val_out1_acc: 0.2797 - val_out2_acc: 0.9163
accuracy:  0.8974
cost:      0.868868776855
Epoch 4/50
 - 50s - loss: 1.1785 - conf0_loss: 1.1386 - conf1_loss: 1.1654 - out0_loss: 1.1386 - out1_loss: 1.1654 - out2_loss: 1.2430 - conf0_acc: 0.8564 - conf1_acc: 0.8066 - out0_acc: 0.1322 - out1_acc: 0.2454 - out2_acc: 0.9317 - val_loss: 1.1920 - val_conf0_loss: 1.1357 - val_conf1_loss: 1.1659 - val_out0_loss: 1.1357 - val_out1_loss: 1.1659 - val_out2_loss: 1.3146 - val_conf0_acc: 0.8467 - val_conf1_acc: 0.8138 - val_out0_acc: 0.1275 - val_out1_acc: 0.2330 - val_out2_acc: 0.8980
accuracy:  0.8975
cost:      0.85935259238
Epoch 5/50
 - 50s - loss: 1.1441 - conf0_loss: 1.1052 - conf1_loss: 1.1301 - out0_loss: 1.1052 - out1_loss: 1.1301 - out2_loss: 1.2075 - conf0_acc: 0.8582 - conf1_acc: 0.8095 - out0_acc: 0.1485 - out1_acc: 0.2064 - out2_acc: 0.9412 - val_loss: 1.1679 - val_conf0_loss: 1.1309 - val_conf1_loss: 1.1524 - val_out0_loss: 1.1309 - val_out1_loss: 1.1524 - val_out2_loss: 1.2301 - val_conf0_acc: 0.8877 - val_conf1_acc: 0.7956 - val_out0_acc: 0.1819 - val_out1_acc: 0.1927 - val_out2_acc: 0.9322
accuracy:  0.9194
cost:      0.889770352335
Epoch 6/50
 - 50s - loss: 1.1198 - conf0_loss: 1.0821 - conf1_loss: 1.1045 - out0_loss: 1.0821 - out1_loss: 1.1045 - out2_loss: 1.1828 - conf0_acc: 0.8621 - conf1_acc: 0.8109 - out0_acc: 0.1570 - out1_acc: 0.1999 - out2_acc: 0.9479 - val_loss: 1.1065 - val_conf0_loss: 1.0669 - val_conf1_loss: 1.0904 - val_out0_loss: 1.0669 - val_out1_loss: 1.0904 - val_out2_loss: 1.1747 - val_conf0_acc: 0.8548 - val_conf1_acc: 0.8147 - val_out0_acc: 0.1430 - val_out1_acc: 0.2060 - val_out2_acc: 0.9493
accuracy:  0.9268
cost:      0.86947224291
Epoch 7/50
 - 50s - loss: 1.1013 - conf0_loss: 1.0639 - conf1_loss: 1.0852 - out0_loss: 1.0639 - out1_loss: 1.0852 - out2_loss: 1.1648 - conf0_acc: 0.8638 - conf1_acc: 0.8115 - out0_acc: 0.1596 - out1_acc: 0.1998 - out2_acc: 0.9524 - val_loss: 1.0809 - val_conf0_loss: 1.0421 - val_conf1_loss: 1.0649 - val_out0_loss: 1.0421 - val_out1_loss: 1.0649 - val_out2_loss: 1.1465 - val_conf0_acc: 0.8756 - val_conf1_acc: 0.8079 - val_out0_acc: 0.1728 - val_out1_acc: 0.2049 - val_out2_acc: 0.9564
accuracy:  0.9423
cost:      0.887183930106
Epoch 8/50
 - 51s - loss: 1.0880 - conf0_loss: 1.0512 - conf1_loss: 1.0714 - out0_loss: 1.0512 - out1_loss: 1.0714 - out2_loss: 1.1509 - conf0_acc: 0.8632 - conf1_acc: 0.8115 - out0_acc: 0.1673 - out1_acc: 0.2004 - out2_acc: 0.9564 - val_loss: 1.0864 - val_conf0_loss: 1.0495 - val_conf1_loss: 1.0691 - val_out0_loss: 1.0495 - val_out1_loss: 1.0691 - val_out2_loss: 1.1509 - val_conf0_acc: 0.8595 - val_conf1_acc: 0.8102 - val_out0_acc: 0.1561 - val_out1_acc: 0.2070 - val_out2_acc: 0.9522
accuracy:  0.9311
cost:      0.87071627041
Epoch 9/50
 - 50s - loss: 1.0805 - conf0_loss: 1.0436 - conf1_loss: 1.0637 - out0_loss: 1.0436 - out1_loss: 1.0637 - out2_loss: 1.1431 - conf0_acc: 0.8635 - conf1_acc: 0.8118 - out0_acc: 0.1664 - out1_acc: 0.2010 - out2_acc: 0.9583 - val_loss: 1.0865 - val_conf0_loss: 1.0477 - val_conf1_loss: 1.0688 - val_out0_loss: 1.0477 - val_out1_loss: 1.0688 - val_out2_loss: 1.1548 - val_conf0_acc: 0.8550 - val_conf1_acc: 0.8070 - val_out0_acc: 0.1609 - val_out1_acc: 0.2038 - val_out2_acc: 0.9533
accuracy:  0.9269
cost:      0.862610627327
Epoch 10/50
 - 50s - loss: 1.0715 - conf0_loss: 1.0348 - conf1_loss: 1.0546 - out0_loss: 1.0348 - out1_loss: 1.0546 - out2_loss: 1.1341 - conf0_acc: 0.8639 - conf1_acc: 0.8120 - out0_acc: 0.1692 - out1_acc: 0.2009 - out2_acc: 0.9615 - val_loss: 1.0501 - val_conf0_loss: 1.0115 - val_conf1_loss: 1.0316 - val_out0_loss: 1.0115 - val_out1_loss: 1.0316 - val_out2_loss: 1.1195 - val_conf0_acc: 0.8539 - val_conf1_acc: 0.8082 - val_out0_acc: 0.1606 - val_out1_acc: 0.2052 - val_out2_acc: 0.9648
accuracy:  0.9396
cost:      0.862423947293
Epoch 11/50
 - 50s - loss: 1.0641 - conf0_loss: 1.0274 - conf1_loss: 1.0470 - out0_loss: 1.0274 - out1_loss: 1.0470 - out2_loss: 1.1267 - conf0_acc: 0.8633 - conf1_acc: 0.8120 - out0_acc: 0.1713 - out1_acc: 0.2010 - out2_acc: 0.9624 - val_loss: 1.0387 - val_conf0_loss: 1.0010 - val_conf1_loss: 1.0203 - val_out0_loss: 1.0010 - val_out1_loss: 1.0203 - val_out2_loss: 1.1057 - val_conf0_acc: 0.8620 - val_conf1_acc: 0.8106 - val_out0_acc: 0.1640 - val_out1_acc: 0.2069 - val_out2_acc: 0.9691
accuracy:  0.9493
cost:      0.874015382412
Epoch 12/50
 - 50s - loss: 1.0582 - conf0_loss: 1.0215 - conf1_loss: 1.0409 - out0_loss: 1.0215 - out1_loss: 1.0409 - out2_loss: 1.1207 - conf0_acc: 0.8643 - conf1_acc: 0.8123 - out0_acc: 0.1697 - out1_acc: 0.2004 - out2_acc: 0.9642 - val_loss: 1.0447 - val_conf0_loss: 1.0080 - val_conf1_loss: 1.0281 - val_out0_loss: 1.0080 - val_out1_loss: 1.0281 - val_out2_loss: 1.1061 - val_conf0_acc: 0.8653 - val_conf1_acc: 0.8046 - val_out0_acc: 0.1765 - val_out1_acc: 0.1988 - val_out2_acc: 0.9660
accuracy:  0.9442
cost:      0.87227556574
Epoch 13/50
 - 51s - loss: 1.0543 - conf0_loss: 1.0178 - conf1_loss: 1.0370 - out0_loss: 1.0178 - out1_loss: 1.0370 - out2_loss: 1.1165 - conf0_acc: 0.8633 - conf1_acc: 0.8122 - out0_acc: 0.1748 - out1_acc: 0.2010 - out2_acc: 0.9658 - val_loss: 1.0606 - val_conf0_loss: 1.0240 - val_conf1_loss: 1.0436 - val_out0_loss: 1.0240 - val_out1_loss: 1.0436 - val_out2_loss: 1.1224 - val_conf0_acc: 0.8686 - val_conf1_acc: 0.8073 - val_out0_acc: 0.1703 - val_out1_acc: 0.2000 - val_out2_acc: 0.9621
accuracy:  0.9437
cost:      0.87857556574
Epoch 14/50
 - 51s - loss: 1.0503 - conf0_loss: 1.0139 - conf1_loss: 1.0327 - out0_loss: 1.0139 - out1_loss: 1.0327 - out2_loss: 1.1126 - conf0_acc: 0.8632 - conf1_acc: 0.8124 - out0_acc: 0.1736 - out1_acc: 0.2008 - out2_acc: 0.9664 - val_loss: 1.0556 - val_conf0_loss: 1.0220 - val_conf1_loss: 1.0364 - val_out0_loss: 1.0220 - val_out1_loss: 1.0364 - val_out2_loss: 1.1155 - val_conf0_acc: 0.8230 - val_conf1_acc: 0.8162 - val_out0_acc: 0.1356 - val_out1_acc: 0.2088 - val_out2_acc: 0.9636
accuracy:  0.9195
cost:      0.834275021484
Epoch 15/50
 - 50s - loss: 1.0463 - conf0_loss: 1.0098 - conf1_loss: 1.0288 - out0_loss: 1.0098 - out1_loss: 1.0288 - out2_loss: 1.1085 - conf0_acc: 0.8634 - conf1_acc: 0.8123 - out0_acc: 0.1737 - out1_acc: 0.2008 - out2_acc: 0.9677 - val_loss: 1.0407 - val_conf0_loss: 1.0025 - val_conf1_loss: 1.0240 - val_out0_loss: 1.0025 - val_out1_loss: 1.0240 - val_out2_loss: 1.1046 - val_conf0_acc: 0.8702 - val_conf1_acc: 0.8101 - val_out0_acc: 0.1764 - val_out1_acc: 0.2050 - val_out2_acc: 0.9673
accuracy:  0.9519
cost:      0.882902606703
Epoch 16/50
 - 50s - loss: 1.0437 - conf0_loss: 1.0075 - conf1_loss: 1.0260 - out0_loss: 1.0075 - out1_loss: 1.0260 - out2_loss: 1.1059 - conf0_acc: 0.8636 - conf1_acc: 0.8124 - out0_acc: 0.1757 - out1_acc: 0.2009 - out2_acc: 0.9681 - val_loss: 1.1565 - val_conf0_loss: 1.0976 - val_conf1_loss: 1.1153 - val_out0_loss: 1.0976 - val_out1_loss: 1.1153 - val_out2_loss: 1.3109 - val_conf0_acc: 0.8921 - val_conf1_acc: 0.7991 - val_out0_acc: 0.1829 - val_out1_acc: 0.1796 - val_out2_acc: 0.8790
accuracy:  0.9293
cost:      0.898078172443
Epoch 17/50
 - 50s - loss: 1.0407 - conf0_loss: 1.0043 - conf1_loss: 1.0230 - out0_loss: 1.0043 - out1_loss: 1.0230 - out2_loss: 1.1031 - conf0_acc: 0.8643 - conf1_acc: 0.8123 - out0_acc: 0.1767 - out1_acc: 0.1990 - out2_acc: 0.9691 - val_loss: 1.0463 - val_conf0_loss: 1.0089 - val_conf1_loss: 1.0290 - val_out0_loss: 1.0089 - val_out1_loss: 1.0290 - val_out2_loss: 1.1098 - val_conf0_acc: 0.8691 - val_conf1_acc: 0.8081 - val_out0_acc: 0.1763 - val_out1_acc: 0.1973 - val_out2_acc: 0.9660
accuracy:  0.9486
cost:      0.879864537382
Epoch 18/50
 - 51s - loss: 1.0326 - conf0_loss: 0.9968 - conf1_loss: 1.0148 - out0_loss: 0.9968 - out1_loss: 1.0148 - out2_loss: 1.0938 - conf0_acc: 0.8648 - conf1_acc: 0.8125 - out0_acc: 0.1763 - out1_acc: 0.1978 - out2_acc: 0.9716 - val_loss: 1.0274 - val_conf0_loss: 0.9917 - val_conf1_loss: 1.0096 - val_out0_loss: 0.9917 - val_out1_loss: 1.0096 - val_out2_loss: 1.0885 - val_conf0_acc: 0.8714 - val_conf1_acc: 0.8066 - val_out0_acc: 0.1814 - val_out1_acc: 0.1951 - val_out2_acc: 0.9722
accuracy:  0.9547
cost:      0.881213148095
Epoch 19/50
 - 51s - loss: 1.0301 - conf0_loss: 0.9941 - conf1_loss: 1.0122 - out0_loss: 0.9941 - out1_loss: 1.0122 - out2_loss: 1.0917 - conf0_acc: 0.8634 - conf1_acc: 0.8125 - out0_acc: 0.1786 - out1_acc: 0.1990 - out2_acc: 0.9724 - val_loss: 1.0153 - val_conf0_loss: 0.9786 - val_conf1_loss: 0.9967 - val_out0_loss: 0.9786 - val_out1_loss: 0.9967 - val_out2_loss: 1.0804 - val_conf0_acc: 0.8714 - val_conf1_acc: 0.8098 - val_out0_acc: 0.1809 - val_out1_acc: 0.2023 - val_out2_acc: 0.9748
accuracy:  0.9603
cost:      0.884083128044
Epoch 20/50
 - 51s - loss: 1.0295 - conf0_loss: 0.9937 - conf1_loss: 1.0117 - out0_loss: 0.9937 - out1_loss: 1.0117 - out2_loss: 1.0910 - conf0_acc: 0.8639 - conf1_acc: 0.8126 - out0_acc: 0.1787 - out1_acc: 0.1981 - out2_acc: 0.9730 - val_loss: 1.0195 - val_conf0_loss: 0.9833 - val_conf1_loss: 1.0019 - val_out0_loss: 0.9833 - val_out1_loss: 1.0019 - val_out2_loss: 1.0815 - val_conf0_acc: 0.8754 - val_conf1_acc: 0.8083 - val_out0_acc: 0.1841 - val_out1_acc: 0.1953 - val_out2_acc: 0.9738
accuracy:  0.9607
cost:      0.887367516471
Epoch 21/50
 - 50s - loss: 1.0291 - conf0_loss: 0.9934 - conf1_loss: 1.0114 - out0_loss: 0.9934 - out1_loss: 1.0114 - out2_loss: 1.0908 - conf0_acc: 0.8632 - conf1_acc: 0.8126 - out0_acc: 0.1771 - out1_acc: 0.1978 - out2_acc: 0.9726 - val_loss: 1.0164 - val_conf0_loss: 0.9784 - val_conf1_loss: 0.9988 - val_out0_loss: 0.9784 - val_out1_loss: 0.9988 - val_out2_loss: 1.0821 - val_conf0_acc: 0.8630 - val_conf1_acc: 0.8097 - val_out0_acc: 0.1726 - val_out1_acc: 0.2014 - val_out2_acc: 0.9740
accuracy:  0.9545
cost:      0.874330650243
Epoch 22/50
 - 51s - loss: 1.0281 - conf0_loss: 0.9922 - conf1_loss: 1.0103 - out0_loss: 0.9922 - out1_loss: 1.0103 - out2_loss: 1.0901 - conf0_acc: 0.8627 - conf1_acc: 0.8125 - out0_acc: 0.1760 - out1_acc: 0.1996 - out2_acc: 0.9724 - val_loss: 1.0126 - val_conf0_loss: 0.9764 - val_conf1_loss: 0.9938 - val_out0_loss: 0.9764 - val_out1_loss: 0.9938 - val_out2_loss: 1.0776 - val_conf0_acc: 0.8698 - val_conf1_acc: 0.8090 - val_out0_acc: 0.1786 - val_out1_acc: 0.2027 - val_out2_acc: 0.9749
accuracy:  0.9588
cost:      0.881577227156
Epoch 23/50
 - 50s - loss: 1.0274 - conf0_loss: 0.9919 - conf1_loss: 1.0096 - out0_loss: 0.9919 - out1_loss: 1.0096 - out2_loss: 1.0890 - conf0_acc: 0.8632 - conf1_acc: 0.8125 - out0_acc: 0.1785 - out1_acc: 0.2000 - out2_acc: 0.9736 - val_loss: 1.0235 - val_conf0_loss: 0.9864 - val_conf1_loss: 1.0061 - val_out0_loss: 0.9864 - val_out1_loss: 1.0061 - val_out2_loss: 1.0877 - val_conf0_acc: 0.8637 - val_conf1_acc: 0.8098 - val_out0_acc: 0.1742 - val_out1_acc: 0.2025 - val_out2_acc: 0.9714
accuracy:  0.9522
cost:      0.875141678602
Epoch 24/50
 - 51s - loss: 1.0260 - conf0_loss: 0.9905 - conf1_loss: 1.0085 - out0_loss: 0.9905 - out1_loss: 1.0085 - out2_loss: 1.0873 - conf0_acc: 0.8638 - conf1_acc: 0.8125 - out0_acc: 0.1777 - out1_acc: 0.2002 - out2_acc: 0.9738 - val_loss: 1.0160 - val_conf0_loss: 0.9778 - val_conf1_loss: 0.9952 - val_out0_loss: 0.9778 - val_out1_loss: 0.9952 - val_out2_loss: 1.0894 - val_conf0_acc: 0.8742 - val_conf1_acc: 0.8108 - val_out0_acc: 0.1814 - val_out1_acc: 0.2063 - val_out2_acc: 0.9734
accuracy:  0.9623
cost:      0.888173904325
Epoch 25/50
 - 50s - loss: 1.0255 - conf0_loss: 0.9901 - conf1_loss: 1.0078 - out0_loss: 0.9901 - out1_loss: 1.0078 - out2_loss: 1.0874 - conf0_acc: 0.8629 - conf1_acc: 0.8126 - out0_acc: 0.1780 - out1_acc: 0.2004 - out2_acc: 0.9732 - val_loss: 1.0281 - val_conf0_loss: 0.9920 - val_conf1_loss: 1.0097 - val_out0_loss: 0.9920 - val_out1_loss: 1.0097 - val_out2_loss: 1.0930 - val_conf0_acc: 0.8643 - val_conf1_acc: 0.8105 - val_out0_acc: 0.1744 - val_out1_acc: 0.2068 - val_out2_acc: 0.9695
accuracy:  0.9509
cost:      0.876541420796
Epoch 26/50
 - 51s - loss: 1.0257 - conf0_loss: 0.9902 - conf1_loss: 1.0083 - out0_loss: 0.9902 - out1_loss: 1.0083 - out2_loss: 1.0874 - conf0_acc: 0.8643 - conf1_acc: 0.8126 - out0_acc: 0.1781 - out1_acc: 0.2006 - out2_acc: 0.9741 - val_loss: 1.0342 - val_conf0_loss: 0.9898 - val_conf1_loss: 1.0109 - val_out0_loss: 0.9898 - val_out1_loss: 1.0109 - val_out2_loss: 1.1254 - val_conf0_acc: 0.8758 - val_conf1_acc: 0.8090 - val_out0_acc: 0.1849 - val_out1_acc: 0.2009 - val_out2_acc: 0.9596
accuracy:  0.9566
cost:      0.888417129762
Epoch 27/50
 - 50s - loss: 1.0237 - conf0_loss: 0.9884 - conf1_loss: 1.0061 - out0_loss: 0.9884 - out1_loss: 1.0061 - out2_loss: 1.0854 - conf0_acc: 0.8651 - conf1_acc: 0.8125 - out0_acc: 0.1786 - out1_acc: 0.2003 - out2_acc: 0.9745 - val_loss: 1.0117 - val_conf0_loss: 0.9753 - val_conf1_loss: 0.9941 - val_out0_loss: 0.9753 - val_out1_loss: 0.9941 - val_out2_loss: 1.0760 - val_conf0_acc: 0.8543 - val_conf1_acc: 0.8102 - val_out0_acc: 0.1690 - val_out1_acc: 0.2041 - val_out2_acc: 0.9769
accuracy:  0.9517
cost:      0.864798166714
Epoch 28/50
 - 51s - loss: 1.0239 - conf0_loss: 0.9885 - conf1_loss: 1.0064 - out0_loss: 0.9885 - out1_loss: 1.0064 - out2_loss: 1.0857 - conf0_acc: 0.8627 - conf1_acc: 0.8127 - out0_acc: 0.1780 - out1_acc: 0.1997 - out2_acc: 0.9742 - val_loss: 1.0251 - val_conf0_loss: 0.9893 - val_conf1_loss: 1.0072 - val_out0_loss: 0.9893 - val_out1_loss: 1.0072 - val_out2_loss: 1.0890 - val_conf0_acc: 0.8685 - val_conf1_acc: 0.8090 - val_out0_acc: 0.1785 - val_out1_acc: 0.2014 - val_out2_acc: 0.9703
accuracy:  0.954
cost:      0.879936579777
Epoch 29/50
 - 51s - loss: 1.0215 - conf0_loss: 0.9864 - conf1_loss: 1.0039 - out0_loss: 0.9864 - out1_loss: 1.0039 - out2_loss: 1.0832 - conf0_acc: 0.8642 - conf1_acc: 0.8128 - out0_acc: 0.1802 - out1_acc: 0.1993 - out2_acc: 0.9746 - val_loss: 1.0216 - val_conf0_loss: 0.9825 - val_conf1_loss: 1.0010 - val_out0_loss: 0.9825 - val_out1_loss: 1.0010 - val_out2_loss: 1.0972 - val_conf0_acc: 0.8703 - val_conf1_acc: 0.8086 - val_out0_acc: 0.1822 - val_out1_acc: 0.2014 - val_out2_acc: 0.9694
accuracy:  0.9568
cost:      0.881688771126
Epoch 30/50
 - 51s - loss: 1.0228 - conf0_loss: 0.9874 - conf1_loss: 1.0055 - out0_loss: 0.9874 - out1_loss: 1.0055 - out2_loss: 1.0847 - conf0_acc: 0.8641 - conf1_acc: 0.8125 - out0_acc: 0.1803 - out1_acc: 0.1998 - out2_acc: 0.9741 - val_loss: 1.0177 - val_conf0_loss: 0.9792 - val_conf1_loss: 0.9987 - val_out0_loss: 0.9792 - val_out1_loss: 0.9987 - val_out2_loss: 1.0893 - val_conf0_acc: 0.8615 - val_conf1_acc: 0.8112 - val_out0_acc: 0.1789 - val_out1_acc: 0.2055 - val_out2_acc: 0.9729
accuracy:  0.9545
cost:      0.873977427671
Epoch 31/50
 - 50s - loss: 1.0216 - conf0_loss: 0.9866 - conf1_loss: 1.0042 - out0_loss: 0.9866 - out1_loss: 1.0042 - out2_loss: 1.0831 - conf0_acc: 0.8636 - conf1_acc: 0.8129 - out0_acc: 0.1792 - out1_acc: 0.1996 - out2_acc: 0.9751 - val_loss: 1.0076 - val_conf0_loss: 0.9709 - val_conf1_loss: 0.9893 - val_out0_loss: 0.9709 - val_out1_loss: 0.9893 - val_out2_loss: 1.0743 - val_conf0_acc: 0.8643 - val_conf1_acc: 0.8097 - val_out0_acc: 0.1806 - val_out1_acc: 0.2023 - val_out2_acc: 0.9764
accuracy:  0.9582
cost:      0.875805528502
Epoch 32/50
 - 50s - loss: 1.0219 - conf0_loss: 0.9867 - conf1_loss: 1.0045 - out0_loss: 0.9867 - out1_loss: 1.0045 - out2_loss: 1.0839 - conf0_acc: 0.8646 - conf1_acc: 0.8127 - out0_acc: 0.1798 - out1_acc: 0.1993 - out2_acc: 0.9746 - val_loss: 1.0087 - val_conf0_loss: 0.9723 - val_conf1_loss: 0.9911 - val_out0_loss: 0.9723 - val_out1_loss: 0.9911 - val_out2_loss: 1.0738 - val_conf0_acc: 0.8651 - val_conf1_acc: 0.8102 - val_out0_acc: 0.1807 - val_out1_acc: 0.2034 - val_out2_acc: 0.9759
accuracy:  0.9579
cost:      0.87716886279
Epoch 33/50
 - 51s - loss: 1.0202 - conf0_loss: 0.9850 - conf1_loss: 1.0029 - out0_loss: 0.9850 - out1_loss: 1.0029 - out2_loss: 1.0823 - conf0_acc: 0.8639 - conf1_acc: 0.8127 - out0_acc: 0.1802 - out1_acc: 0.1987 - out2_acc: 0.9756 - val_loss: 1.0075 - val_conf0_loss: 0.9710 - val_conf1_loss: 0.9903 - val_out0_loss: 0.9710 - val_out1_loss: 0.9903 - val_out2_loss: 1.0724 - val_conf0_acc: 0.8651 - val_conf1_acc: 0.8098 - val_out0_acc: 0.1804 - val_out1_acc: 0.2019 - val_out2_acc: 0.9773
accuracy:  0.9591
cost:      0.876874505872
Epoch 34/50
 - 51s - loss: 1.0196 - conf0_loss: 0.9846 - conf1_loss: 1.0023 - out0_loss: 0.9846 - out1_loss: 1.0023 - out2_loss: 1.0815 - conf0_acc: 0.8637 - conf1_acc: 0.8128 - out0_acc: 0.1812 - out1_acc: 0.1991 - out2_acc: 0.9748 - val_loss: 1.0137 - val_conf0_loss: 0.9774 - val_conf1_loss: 0.9962 - val_out0_loss: 0.9774 - val_out1_loss: 0.9962 - val_out2_loss: 1.0789 - val_conf0_acc: 0.8662 - val_conf1_acc: 0.8089 - val_out0_acc: 0.1785 - val_out1_acc: 0.2026 - val_out2_acc: 0.9753
accuracy:  0.9578
cost:      0.877281953595
Epoch 35/50
 - 51s - loss: 1.0185 - conf0_loss: 0.9835 - conf1_loss: 1.0013 - out0_loss: 0.9835 - out1_loss: 1.0013 - out2_loss: 1.0807 - conf0_acc: 0.8637 - conf1_acc: 0.8127 - out0_acc: 0.1805 - out1_acc: 0.1997 - out2_acc: 0.9757 - val_loss: 1.0121 - val_conf0_loss: 0.9759 - val_conf1_loss: 0.9944 - val_out0_loss: 0.9759 - val_out1_loss: 0.9944 - val_out2_loss: 1.0775 - val_conf0_acc: 0.8660 - val_conf1_acc: 0.8089 - val_out0_acc: 0.1801 - val_out1_acc: 0.1996 - val_out2_acc: 0.9759
accuracy:  0.9579
cost:      0.877005413922
Epoch 36/50
 - 50s - loss: 1.0185 - conf0_loss: 0.9837 - conf1_loss: 1.0012 - out0_loss: 0.9837 - out1_loss: 1.0012 - out2_loss: 1.0801 - conf0_acc: 0.8639 - conf1_acc: 0.8127 - out0_acc: 0.1801 - out1_acc: 0.1996 - out2_acc: 0.9756 - val_loss: 1.0085 - val_conf0_loss: 0.9726 - val_conf1_loss: 0.9912 - val_out0_loss: 0.9726 - val_out1_loss: 0.9912 - val_out2_loss: 1.0727 - val_conf0_acc: 0.8692 - val_conf1_acc: 0.8107 - val_out0_acc: 0.1812 - val_out1_acc: 0.2027 - val_out2_acc: 0.9771
accuracy:  0.9612
cost:      0.882385161845
Epoch 37/50
 - 50s - loss: 1.0185 - conf0_loss: 0.9837 - conf1_loss: 1.0012 - out0_loss: 0.9837 - out1_loss: 1.0012 - out2_loss: 1.0807 - conf0_acc: 0.8649 - conf1_acc: 0.8127 - out0_acc: 0.1809 - out1_acc: 0.1988 - out2_acc: 0.9752 - val_loss: 1.0102 - val_conf0_loss: 0.9739 - val_conf1_loss: 0.9930 - val_out0_loss: 0.9739 - val_out1_loss: 0.9930 - val_out2_loss: 1.0753 - val_conf0_acc: 0.8661 - val_conf1_acc: 0.8098 - val_out0_acc: 0.1782 - val_out1_acc: 0.2009 - val_out2_acc: 0.9761
accuracy:  0.9585
cost:      0.877907075337
Epoch 38/50
 - 51s - loss: 1.0164 - conf0_loss: 0.9816 - conf1_loss: 0.9991 - out0_loss: 0.9816 - out1_loss: 0.9991 - out2_loss: 1.0783 - conf0_acc: 0.8655 - conf1_acc: 0.8126 - out0_acc: 0.1799 - out1_acc: 0.1986 - out2_acc: 0.9764 - val_loss: 1.0032 - val_conf0_loss: 0.9670 - val_conf1_loss: 0.9858 - val_out0_loss: 0.9670 - val_out1_loss: 0.9858 - val_out2_loss: 1.0686 - val_conf0_acc: 0.8630 - val_conf1_acc: 0.8102 - val_out0_acc: 0.1787 - val_out1_acc: 0.2035 - val_out2_acc: 0.9775
accuracy:  0.9583
cost:      0.87477218562
Epoch 39/50
 - 50s - loss: 1.0154 - conf0_loss: 0.9806 - conf1_loss: 0.9982 - out0_loss: 0.9806 - out1_loss: 0.9982 - out2_loss: 1.0775 - conf0_acc: 0.8637 - conf1_acc: 0.8128 - out0_acc: 0.1822 - out1_acc: 0.1987 - out2_acc: 0.9768 - val_loss: 1.0036 - val_conf0_loss: 0.9679 - val_conf1_loss: 0.9858 - val_out0_loss: 0.9679 - val_out1_loss: 0.9858 - val_out2_loss: 1.0690 - val_conf0_acc: 0.8683 - val_conf1_acc: 0.8104 - val_out0_acc: 0.1818 - val_out1_acc: 0.2034 - val_out2_acc: 0.9774
accuracy:  0.9614
cost:      0.881076826124
Epoch 40/50
 - 51s - loss: 1.0151 - conf0_loss: 0.9803 - conf1_loss: 0.9980 - out0_loss: 0.9803 - out1_loss: 0.9980 - out2_loss: 1.0772 - conf0_acc: 0.8642 - conf1_acc: 0.8127 - out0_acc: 0.1817 - out1_acc: 0.1989 - out2_acc: 0.9767 - val_loss: 1.0027 - val_conf0_loss: 0.9667 - val_conf1_loss: 0.9852 - val_out0_loss: 0.9667 - val_out1_loss: 0.9852 - val_out2_loss: 1.0679 - val_conf0_acc: 0.8645 - val_conf1_acc: 0.8106 - val_out0_acc: 0.1800 - val_out1_acc: 0.2037 - val_out2_acc: 0.9784
accuracy:  0.96
cost:      0.876799369808
Epoch 41/50
 - 51s - loss: 1.0153 - conf0_loss: 0.9804 - conf1_loss: 0.9982 - out0_loss: 0.9804 - out1_loss: 0.9982 - out2_loss: 1.0774 - conf0_acc: 0.8651 - conf1_acc: 0.8127 - out0_acc: 0.1812 - out1_acc: 0.1989 - out2_acc: 0.9767 - val_loss: 1.0029 - val_conf0_loss: 0.9671 - val_conf1_loss: 0.9854 - val_out0_loss: 0.9671 - val_out1_loss: 0.9854 - val_out2_loss: 1.0681 - val_conf0_acc: 0.8685 - val_conf1_acc: 0.8106 - val_out0_acc: 0.1825 - val_out1_acc: 0.2037 - val_out2_acc: 0.9779
accuracy:  0.9619
cost:      0.881481953595
Epoch 42/50
 - 51s - loss: 1.0156 - conf0_loss: 0.9805 - conf1_loss: 0.9987 - out0_loss: 0.9805 - out1_loss: 0.9987 - out2_loss: 1.0780 - conf0_acc: 0.8635 - conf1_acc: 0.8128 - out0_acc: 0.1817 - out1_acc: 0.1992 - out2_acc: 0.9764 - val_loss: 1.0027 - val_conf0_loss: 0.9665 - val_conf1_loss: 0.9855 - val_out0_loss: 0.9665 - val_out1_loss: 0.9855 - val_out2_loss: 1.0679 - val_conf0_acc: 0.8669 - val_conf1_acc: 0.8094 - val_out0_acc: 0.1815 - val_out1_acc: 0.2021 - val_out2_acc: 0.9778
accuracy:  0.9605
cost:      0.87853451733
Epoch 43/50
 - 50s - loss: 1.0158 - conf0_loss: 0.9811 - conf1_loss: 0.9986 - out0_loss: 0.9811 - out1_loss: 0.9986 - out2_loss: 1.0778 - conf0_acc: 0.8636 - conf1_acc: 0.8125 - out0_acc: 0.1803 - out1_acc: 0.1987 - out2_acc: 0.9764 - val_loss: 1.0042 - val_conf0_loss: 0.9685 - val_conf1_loss: 0.9867 - val_out0_loss: 0.9685 - val_out1_loss: 0.9867 - val_out2_loss: 1.0689 - val_conf0_acc: 0.8706 - val_conf1_acc: 0.8094 - val_out0_acc: 0.1838 - val_out1_acc: 0.2013 - val_out2_acc: 0.9783
accuracy:  0.9632
cost:      0.882774792323
Epoch 44/50
 - 51s - loss: 1.0140 - conf0_loss: 0.9793 - conf1_loss: 0.9968 - out0_loss: 0.9793 - out1_loss: 0.9968 - out2_loss: 1.0760 - conf0_acc: 0.8637 - conf1_acc: 0.8127 - out0_acc: 0.1804 - out1_acc: 0.1991 - out2_acc: 0.9767 - val_loss: 1.0037 - val_conf0_loss: 0.9678 - val_conf1_loss: 0.9864 - val_out0_loss: 0.9678 - val_out1_loss: 0.9864 - val_out2_loss: 1.0686 - val_conf0_acc: 0.8670 - val_conf1_acc: 0.8099 - val_out0_acc: 0.1812 - val_out1_acc: 0.2032 - val_out2_acc: 0.9778
accuracy:  0.9606
cost:      0.879160412489
Epoch 45/50
 - 50s - loss: 1.0142 - conf0_loss: 0.9792 - conf1_loss: 0.9972 - out0_loss: 0.9792 - out1_loss: 0.9972 - out2_loss: 1.0765 - conf0_acc: 0.8639 - conf1_acc: 0.8126 - out0_acc: 0.1814 - out1_acc: 0.1991 - out2_acc: 0.9771 - val_loss: 1.0024 - val_conf0_loss: 0.9666 - val_conf1_loss: 0.9848 - val_out0_loss: 0.9666 - val_out1_loss: 0.9848 - val_out2_loss: 1.0675 - val_conf0_acc: 0.8694 - val_conf1_acc: 0.8098 - val_out0_acc: 0.1835 - val_out1_acc: 0.2027 - val_out2_acc: 0.9788
accuracy:  0.9632
cost:      0.881778630765
Epoch 46/50
 - 50s - loss: 1.0140 - conf0_loss: 0.9792 - conf1_loss: 0.9969 - out0_loss: 0.9792 - out1_loss: 0.9969 - out2_loss: 1.0766 - conf0_acc: 0.8650 - conf1_acc: 0.8125 - out0_acc: 0.1823 - out1_acc: 0.1991 - out2_acc: 0.9772 - val_loss: 1.0066 - val_conf0_loss: 0.9711 - val_conf1_loss: 0.9888 - val_out0_loss: 0.9711 - val_out1_loss: 0.9888 - val_out2_loss: 1.0720 - val_conf0_acc: 0.8638 - val_conf1_acc: 0.8110 - val_out0_acc: 0.1805 - val_out1_acc: 0.2049 - val_out2_acc: 0.9773
accuracy:  0.9589
cost:      0.876429876826
Epoch 47/50
 - 51s - loss: 1.0131 - conf0_loss: 0.9785 - conf1_loss: 0.9960 - out0_loss: 0.9785 - out1_loss: 0.9960 - out2_loss: 1.0751 - conf0_acc: 0.8633 - conf1_acc: 0.8127 - out0_acc: 0.1821 - out1_acc: 0.1995 - out2_acc: 0.9772 - val_loss: 1.0025 - val_conf0_loss: 0.9664 - val_conf1_loss: 0.9851 - val_out0_loss: 0.9664 - val_out1_loss: 0.9851 - val_out2_loss: 1.0681 - val_conf0_acc: 0.8665 - val_conf1_acc: 0.8105 - val_out0_acc: 0.1815 - val_out1_acc: 0.2040 - val_out2_acc: 0.9790
accuracy:  0.9615
cost:      0.879030277857
Epoch 48/50
 - 50s - loss: 1.0148 - conf0_loss: 0.9801 - conf1_loss: 0.9979 - out0_loss: 0.9801 - out1_loss: 0.9979 - out2_loss: 1.0769 - conf0_acc: 0.8634 - conf1_acc: 0.8128 - out0_acc: 0.1820 - out1_acc: 0.1998 - out2_acc: 0.9769 - val_loss: 1.0022 - val_conf0_loss: 0.9664 - val_conf1_loss: 0.9850 - val_out0_loss: 0.9664 - val_out1_loss: 0.9850 - val_out2_loss: 1.0670 - val_conf0_acc: 0.8658 - val_conf1_acc: 0.8095 - val_out0_acc: 0.1808 - val_out1_acc: 0.2023 - val_out2_acc: 0.9785
accuracy:  0.9602
cost:      0.877317588084
Epoch 49/50
 - 51s - loss: 1.0122 - conf0_loss: 0.9775 - conf1_loss: 0.9950 - out0_loss: 0.9775 - out1_loss: 0.9950 - out2_loss: 1.0746 - conf0_acc: 0.8641 - conf1_acc: 0.8127 - out0_acc: 0.1813 - out1_acc: 0.1994 - out2_acc: 0.9778 - val_loss: 1.0036 - val_conf0_loss: 0.9671 - val_conf1_loss: 0.9863 - val_out0_loss: 0.9671 - val_out1_loss: 0.9863 - val_out2_loss: 1.0703 - val_conf0_acc: 0.8660 - val_conf1_acc: 0.8097 - val_out0_acc: 0.1815 - val_out1_acc: 0.2036 - val_out2_acc: 0.9772
accuracy:  0.9603
cost:      0.877741306216
Epoch 50/50
 - 51s - loss: 1.0131 - conf0_loss: 0.9784 - conf1_loss: 0.9961 - out0_loss: 0.9784 - out1_loss: 0.9961 - out2_loss: 1.0755 - conf0_acc: 0.8639 - conf1_acc: 0.8125 - out0_acc: 0.1821 - out1_acc: 0.1996 - out2_acc: 0.9771 - val_loss: 1.0026 - val_conf0_loss: 0.9671 - val_conf1_loss: 0.9855 - val_out0_loss: 0.9671 - val_out1_loss: 0.9855 - val_out2_loss: 1.0668 - val_conf0_acc: 0.8674 - val_conf1_acc: 0.8090 - val_out0_acc: 0.1829 - val_out1_acc: 0.2020 - val_out2_acc: 0.9781
accuracy:  0.961
cost:      0.87871965053

T: 0.50 --> acc: 0.961000 cost: 0.878720
out0        546 0.08%
out1       1844 0.26%
out2       6982 1.00%

Exit Distribution: <408, 1137, 8455>
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 28, 28, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 28, 28, 2)    20          input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 28, 28, 2)    8           conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 28, 28, 2)    0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 28, 28, 2)    38          activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 28, 28, 2)    8           conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 28, 28, 2)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 28, 2)    38          activation_2[0][0]               
__________________________________________________________________________________________________
add_1 (Add)                     (None, 28, 28, 2)    0           activation_1[0][0]               
                                                                 conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 28, 2)    8           add_1[0][0]                      
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 28, 2)    0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 14, 14, 4)    76          activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 14, 14, 4)    16          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 14, 14, 4)    0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 14, 4)    12          add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 14, 4)    148         activation_4[0][0]               
__________________________________________________________________________________________________
add_2 (Add)                     (None, 14, 14, 4)    0           conv2d_6[0][0]                   
                                                                 conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 14, 4)    16          add_2[0][0]                      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 14, 4)    0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 7, 7, 8)      296         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 7, 7, 8)      32          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 7, 7, 8)      0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 7, 7, 8)      40          add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 7, 7, 8)      584         activation_6[0][0]               
__________________________________________________________________________________________________
add_3 (Add)                     (None, 7, 7, 8)      0           conv2d_9[0][0]                   
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 7, 7, 8)      32          add_3[0][0]                      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 7, 7, 8)      0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 1, 1, 2)      0           add_1[0][0]                      
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 1, 1, 4)      0           add_2[0][0]                      
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 1, 1, 8)      0           activation_7[0][0]               
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 2)            0           average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 4)            0           average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 8)            0           average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
conf0 (Dense)                   (None, 1)            3           flatten_2[0][0]                  
__________________________________________________________________________________________________
conf1 (Dense)                   (None, 1)            5           flatten_3[0][0]                  
__________________________________________________________________________________________________
out0 (Dense)                    (None, 10)           30          flatten_2[0][0]                  
__________________________________________________________________________________________________
out1 (Dense)                    (None, 10)           50          flatten_3[0][0]                  
__________________________________________________________________________________________________
out2 (Dense)                    (None, 10)           90          flatten_1[0][0]                  
==================================================================================================
Total params: 1,550
Trainable params: 1,490
Non-trainable params: 60
__________________________________________________________________________________________________
