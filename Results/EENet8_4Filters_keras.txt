Using mnist dataset
Not using data augmentation.
Train on 60000 samples, validate on 10000 samples
Epoch 1/30
 - 66s - loss: 26.4999 - conf0_loss: 5.2967 - conf1_loss: 5.2967 - out0_loss: 5.2967 - out1_loss: 5.2967 - out2_loss: 5.2967 - conf0_acc: 0.7794 - conf1_acc: 0.2848 - out0_acc: 0.0955 - out1_acc: 0.4390 - out2_acc: 0.7352 - val_loss: 17.9766 - val_conf0_loss: 3.5915 - val_conf1_loss: 3.5915 - val_out0_loss: 3.5915 - val_out1_loss: 3.5915 - val_out2_loss: 3.5915 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.6636 - val_out0_acc: 0.0967 - val_out1_acc: 0.5225 - val_out2_acc: 0.9280
accuracy:  0.8675
cost:      0.780143604246
Epoch 2/30
 - 62s - loss: 17.1260 - conf0_loss: 3.4211 - conf1_loss: 3.4211 - out0_loss: 3.4211 - out1_loss: 3.4211 - out2_loss: 3.4211 - conf0_acc: 0.9000 - conf1_acc: 0.6353 - out0_acc: 0.0980 - out1_acc: 0.5038 - out2_acc: 0.9401 - val_loss: 17.4190 - val_conf0_loss: 3.4795 - val_conf1_loss: 3.4795 - val_out0_loss: 3.4795 - val_out1_loss: 3.4795 - val_out2_loss: 3.4795 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.6394 - val_out0_acc: 0.0976 - val_out1_acc: 0.5268 - val_out2_acc: 0.9189
accuracy:  0.8754
cost:      0.757674355001
Epoch 3/30
 - 61s - loss: 15.7699 - conf0_loss: 3.1494 - conf1_loss: 3.1494 - out0_loss: 3.1494 - out1_loss: 3.1494 - out2_loss: 3.1494 - conf0_acc: 0.9000 - conf1_acc: 0.5577 - out0_acc: 0.0978 - out1_acc: 0.5572 - out2_acc: 0.9539 - val_loss: 15.2550 - val_conf0_loss: 3.0463 - val_conf1_loss: 3.0463 - val_out0_loss: 3.0463 - val_out1_loss: 3.0463 - val_out2_loss: 3.0463 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.5249 - val_out0_acc: 0.0979 - val_out1_acc: 0.6343 - val_out2_acc: 0.9526
accuracy:  0.8891
cost:      0.651131424808
Epoch 4/30
 - 61s - loss: 14.9226 - conf0_loss: 2.9796 - conf1_loss: 2.9796 - out0_loss: 2.9796 - out1_loss: 2.9796 - out2_loss: 2.9796 - conf0_acc: 0.9000 - conf1_acc: 0.4756 - out0_acc: 0.0984 - out1_acc: 0.6337 - out2_acc: 0.9609 - val_loss: 14.1313 - val_conf0_loss: 2.8211 - val_conf1_loss: 2.8211 - val_out0_loss: 2.8211 - val_out1_loss: 2.8211 - val_out2_loss: 2.8211 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.3830 - val_out0_acc: 0.0981 - val_out1_acc: 0.6833 - val_out2_acc: 0.9630
accuracy:  0.861
cost:      0.519217587356
Epoch 5/30
 - 61s - loss: 14.3004 - conf0_loss: 2.8548 - conf1_loss: 2.8548 - out0_loss: 2.8548 - out1_loss: 2.8548 - out2_loss: 2.8548 - conf0_acc: 0.9000 - conf1_acc: 0.4131 - out0_acc: 0.0991 - out1_acc: 0.6681 - out2_acc: 0.9644 - val_loss: 15.2328 - val_conf0_loss: 3.0411 - val_conf1_loss: 3.0411 - val_out0_loss: 3.0411 - val_out1_loss: 3.0411 - val_out2_loss: 3.0411 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.4105 - val_out0_acc: 0.1060 - val_out1_acc: 0.5969 - val_out2_acc: 0.9636
accuracy:  0.8314
cost:      0.54473729759
Epoch 6/30
 - 61s - loss: 13.9210 - conf0_loss: 2.7786 - conf1_loss: 2.7786 - out0_loss: 2.7786 - out1_loss: 2.7786 - out2_loss: 2.7786 - conf0_acc: 0.9000 - conf1_acc: 0.3899 - out0_acc: 0.0772 - out1_acc: 0.6891 - out2_acc: 0.9666 - val_loss: 15.0043 - val_conf0_loss: 2.9951 - val_conf1_loss: 2.9951 - val_out0_loss: 2.9951 - val_out1_loss: 2.9951 - val_out2_loss: 2.9951 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.4869 - val_out0_acc: 0.0648 - val_out1_acc: 0.6680 - val_out2_acc: 0.9594
accuracy:  0.8706
cost:      0.615790718215
Epoch 7/30
 - 61s - loss: 13.6267 - conf0_loss: 2.7194 - conf1_loss: 2.7194 - out0_loss: 2.7194 - out1_loss: 2.7194 - out2_loss: 2.7194 - conf0_acc: 0.9000 - conf1_acc: 0.3741 - out0_acc: 0.0573 - out1_acc: 0.7034 - out2_acc: 0.9694 - val_loss: 15.5547 - val_conf0_loss: 3.1049 - val_conf1_loss: 3.1049 - val_out0_loss: 3.1049 - val_out1_loss: 3.1049 - val_out2_loss: 3.1049 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.2894 - val_out0_acc: 0.0493 - val_out1_acc: 0.6305 - val_out2_acc: 0.9552
accuracy:  0.7695
cost:      0.432167846905
Epoch 8/30
 - 61s - loss: 13.3573 - conf0_loss: 2.6653 - conf1_loss: 2.6653 - out0_loss: 2.6653 - out1_loss: 2.6653 - out2_loss: 2.6653 - conf0_acc: 0.9000 - conf1_acc: 0.3496 - out0_acc: 0.0514 - out1_acc: 0.7193 - out2_acc: 0.9710 - val_loss: 12.8797 - val_conf0_loss: 2.5696 - val_conf1_loss: 2.5696 - val_out0_loss: 2.5696 - val_out1_loss: 2.5696 - val_out2_loss: 2.5696 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.3769 - val_out0_acc: 0.0489 - val_out1_acc: 0.7481 - val_out2_acc: 0.9730
accuracy:  0.9102
cost:      0.513488672813
Epoch 9/30
 - 61s - loss: 13.1794 - conf0_loss: 2.6294 - conf1_loss: 2.6294 - out0_loss: 2.6294 - out1_loss: 2.6294 - out2_loss: 2.6294 - conf0_acc: 0.9000 - conf1_acc: 0.3236 - out0_acc: 0.0707 - out1_acc: 0.7375 - out2_acc: 0.9720 - val_loss: 13.0983 - val_conf0_loss: 2.6131 - val_conf1_loss: 2.6131 - val_out0_loss: 2.6131 - val_out1_loss: 2.6131 - val_out2_loss: 2.6131 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.3145 - val_out0_acc: 0.0925 - val_out1_acc: 0.7501 - val_out2_acc: 0.9671
accuracy:  0.88
cost:      0.455455512513
Epoch 10/30
 - 61s - loss: 12.9640 - conf0_loss: 2.5861 - conf1_loss: 2.5861 - out0_loss: 2.5861 - out1_loss: 2.5861 - out2_loss: 2.5861 - conf0_acc: 0.9000 - conf1_acc: 0.2889 - out0_acc: 0.0823 - out1_acc: 0.7563 - out2_acc: 0.9732 - val_loss: 12.6293 - val_conf0_loss: 2.5191 - val_conf1_loss: 2.5191 - val_out0_loss: 2.5191 - val_out1_loss: 2.5191 - val_out2_loss: 2.5191 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.2984 - val_out0_acc: 0.0932 - val_out1_acc: 0.7896 - val_out2_acc: 0.9732
accuracy:  0.8892
cost:      0.440500813512
Epoch 11/30
 - 61s - loss: 12.7674 - conf0_loss: 2.5465 - conf1_loss: 2.5465 - out0_loss: 2.5465 - out1_loss: 2.5465 - out2_loss: 2.5465 - conf0_acc: 0.9000 - conf1_acc: 0.2392 - out0_acc: 0.0849 - out1_acc: 0.7770 - out2_acc: 0.9746 - val_loss: 13.1235 - val_conf0_loss: 2.6176 - val_conf1_loss: 2.6176 - val_out0_loss: 2.6176 - val_out1_loss: 2.6176 - val_out2_loss: 2.6176 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.2762 - val_out0_acc: 0.0994 - val_out1_acc: 0.7737 - val_out2_acc: 0.9677
accuracy:  0.8574
cost:      0.419891601457
Epoch 12/30
 - 61s - loss: 12.5748 - conf0_loss: 2.5078 - conf1_loss: 2.5078 - out0_loss: 2.5078 - out1_loss: 2.5078 - out2_loss: 2.5078 - conf0_acc: 0.9000 - conf1_acc: 0.2025 - out0_acc: 0.0902 - out1_acc: 0.7924 - out2_acc: 0.9749 - val_loss: 12.1243 - val_conf0_loss: 2.4176 - val_conf1_loss: 2.4176 - val_out0_loss: 2.4176 - val_out1_loss: 2.4176 - val_out2_loss: 2.4176 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1685 - val_out0_acc: 0.0933 - val_out1_acc: 0.8248 - val_out2_acc: 0.9732
accuracy:  0.8581
cost:      0.319672797707
Epoch 13/30
 - 61s - loss: 12.4243 - conf0_loss: 2.4774 - conf1_loss: 2.4774 - out0_loss: 2.4774 - out1_loss: 2.4774 - out2_loss: 2.4774 - conf0_acc: 0.9000 - conf1_acc: 0.1747 - out0_acc: 0.0829 - out1_acc: 0.8022 - out2_acc: 0.9753 - val_loss: 12.0149 - val_conf0_loss: 2.3954 - val_conf1_loss: 2.3954 - val_out0_loss: 2.3954 - val_out1_loss: 2.3954 - val_out2_loss: 2.3954 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1247 - val_out0_acc: 0.0982 - val_out1_acc: 0.8306 - val_out2_acc: 0.9755
accuracy:  0.8433
cost:      0.278975184009
Epoch 14/30
 - 61s - loss: 12.2300 - conf0_loss: 2.4383 - conf1_loss: 2.4383 - out0_loss: 2.4383 - out1_loss: 2.4383 - out2_loss: 2.4383 - conf0_acc: 0.9000 - conf1_acc: 0.1557 - out0_acc: 0.1045 - out1_acc: 0.8164 - out2_acc: 0.9759 - val_loss: 12.0642 - val_conf0_loss: 2.4051 - val_conf1_loss: 2.4051 - val_out0_loss: 2.4051 - val_out1_loss: 2.4051 - val_out2_loss: 2.4051 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1326 - val_out0_acc: 0.1009 - val_out1_acc: 0.8310 - val_out2_acc: 0.9751
accuracy:  0.847
cost:      0.28626652979
Epoch 15/30
 - 61s - loss: 12.0221 - conf0_loss: 2.3965 - conf1_loss: 2.3965 - out0_loss: 2.3965 - out1_loss: 2.3965 - out2_loss: 2.3965 - conf0_acc: 0.9000 - conf1_acc: 0.1388 - out0_acc: 0.1030 - out1_acc: 0.8291 - out2_acc: 0.9767 - val_loss: 12.1402 - val_conf0_loss: 2.4200 - val_conf1_loss: 2.4200 - val_out0_loss: 2.4200 - val_out1_loss: 2.4200 - val_out2_loss: 2.4200 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1290 - val_out0_acc: 0.1008 - val_out1_acc: 0.8401 - val_out2_acc: 0.9722
accuracy:  0.8531
cost:      0.282992864337
Epoch 16/30
 - 60s - loss: 11.8355 - conf0_loss: 2.3590 - conf1_loss: 2.3590 - out0_loss: 2.3590 - out1_loss: 2.3590 - out2_loss: 2.3590 - conf0_acc: 0.9000 - conf1_acc: 0.1286 - out0_acc: 0.1021 - out1_acc: 0.8412 - out2_acc: 0.9780 - val_loss: 11.7596 - val_conf0_loss: 2.3437 - val_conf1_loss: 2.3437 - val_out0_loss: 2.3437 - val_out1_loss: 2.3437 - val_out2_loss: 2.3437 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1178 - val_out0_acc: 0.1009 - val_out1_acc: 0.8438 - val_out2_acc: 0.9745
accuracy:  0.8531
cost:      0.272576656078
Epoch 17/30
 - 62s - loss: 11.6665 - conf0_loss: 2.3250 - conf1_loss: 2.3250 - out0_loss: 2.3250 - out1_loss: 2.3250 - out2_loss: 2.3250 - conf0_acc: 0.9000 - conf1_acc: 0.1183 - out0_acc: 0.1021 - out1_acc: 0.8511 - out2_acc: 0.9784 - val_loss: 12.2349 - val_conf0_loss: 2.4386 - val_conf1_loss: 2.4386 - val_out0_loss: 2.4386 - val_out1_loss: 2.4386 - val_out2_loss: 2.4386 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1088 - val_out0_acc: 0.1009 - val_out1_acc: 0.8243 - val_out2_acc: 0.9780
accuracy:  0.8286
cost:      0.264169287983
Epoch 18/30
 - 61s - loss: 11.5391 - conf0_loss: 2.2993 - conf1_loss: 2.2993 - out0_loss: 2.2993 - out1_loss: 2.2993 - out2_loss: 2.2993 - conf0_acc: 0.9000 - conf1_acc: 0.1133 - out0_acc: 0.1022 - out1_acc: 0.8593 - out2_acc: 0.9787 - val_loss: 11.8082 - val_conf0_loss: 2.3531 - val_conf1_loss: 2.3531 - val_out0_loss: 2.3531 - val_out1_loss: 2.3531 - val_out2_loss: 2.3531 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1080 - val_out0_acc: 0.1010 - val_out1_acc: 0.8469 - val_out2_acc: 0.9779
accuracy:  0.8503
cost:      0.263425273108
Epoch 19/30
 - 61s - loss: 11.3745 - conf0_loss: 2.2663 - conf1_loss: 2.2663 - out0_loss: 2.2663 - out1_loss: 2.2663 - out2_loss: 2.2663 - conf0_acc: 0.9000 - conf1_acc: 0.1088 - out0_acc: 0.1022 - out1_acc: 0.8678 - out2_acc: 0.9792 - val_loss: 12.9257 - val_conf0_loss: 2.5764 - val_conf1_loss: 2.5764 - val_out0_loss: 2.5764 - val_out1_loss: 2.5764 - val_out2_loss: 2.5764 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1083 - val_out0_acc: 0.1010 - val_out1_acc: 0.8093 - val_out2_acc: 0.9693
accuracy:  0.8131
cost:      0.263722879058
Epoch 20/30
 - 61s - loss: 11.3023 - conf0_loss: 2.2517 - conf1_loss: 2.2517 - out0_loss: 2.2517 - out1_loss: 2.2517 - out2_loss: 2.2517 - conf0_acc: 0.9000 - conf1_acc: 0.1057 - out0_acc: 0.1022 - out1_acc: 0.8723 - out2_acc: 0.9793 - val_loss: 10.8950 - val_conf0_loss: 2.1701 - val_conf1_loss: 2.1701 - val_out0_loss: 2.1701 - val_out1_loss: 2.1701 - val_out2_loss: 2.1701 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1034 - val_out0_acc: 0.1010 - val_out1_acc: 0.8949 - val_out2_acc: 0.9783
accuracy:  0.897
cost:      0.259109986829
Epoch 21/30
 - 61s - loss: 11.1609 - conf0_loss: 2.2232 - conf1_loss: 2.2232 - out0_loss: 2.2232 - out1_loss: 2.2232 - out2_loss: 2.2232 - conf0_acc: 0.9000 - conf1_acc: 0.1042 - out0_acc: 0.1022 - out1_acc: 0.8777 - out2_acc: 0.9802 - val_loss: 12.7235 - val_conf0_loss: 2.5357 - val_conf1_loss: 2.5357 - val_out0_loss: 2.5357 - val_out1_loss: 2.5357 - val_out2_loss: 2.5357 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1002 - val_out0_acc: 0.1010 - val_out1_acc: 0.8214 - val_out2_acc: 0.9746
accuracy:  0.8215
cost:      0.256208328814
Epoch 22/30
 - 61s - loss: 11.0504 - conf0_loss: 2.2010 - conf1_loss: 2.2010 - out0_loss: 2.2010 - out1_loss: 2.2010 - out2_loss: 2.2010 - conf0_acc: 0.9000 - conf1_acc: 0.1025 - out0_acc: 0.1022 - out1_acc: 0.8817 - out2_acc: 0.9811 - val_loss: 11.4401 - val_conf0_loss: 2.2788 - val_conf1_loss: 2.2788 - val_out0_loss: 2.2788 - val_out1_loss: 2.2788 - val_out2_loss: 2.2788 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1016 - val_out0_acc: 0.1010 - val_out1_acc: 0.8708 - val_out2_acc: 0.9798
accuracy:  0.8719
cost:      0.257473154102
Epoch 23/30
 - 61s - loss: 10.9885 - conf0_loss: 2.1885 - conf1_loss: 2.1885 - out0_loss: 2.1885 - out1_loss: 2.1885 - out2_loss: 2.1885 - conf0_acc: 0.9000 - conf1_acc: 0.1018 - out0_acc: 0.1022 - out1_acc: 0.8860 - out2_acc: 0.9813 - val_loss: 14.2839 - val_conf0_loss: 2.8475 - val_conf1_loss: 2.8475 - val_out0_loss: 2.8475 - val_out1_loss: 2.8475 - val_out2_loss: 2.8475 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1004 - val_out0_acc: 0.1010 - val_out1_acc: 0.7623 - val_out2_acc: 0.9723
accuracy:  0.7628
cost:      0.256357131789
Epoch 24/30
 - 61s - loss: 10.9285 - conf0_loss: 2.1763 - conf1_loss: 2.1763 - out0_loss: 2.1763 - out1_loss: 2.1763 - out2_loss: 2.1763 - conf0_acc: 0.9000 - conf1_acc: 0.1014 - out0_acc: 0.1022 - out1_acc: 0.8885 - out2_acc: 0.9818 - val_loss: 11.8611 - val_conf0_loss: 2.3628 - val_conf1_loss: 2.3628 - val_out0_loss: 2.3628 - val_out1_loss: 2.3628 - val_out2_loss: 2.3628 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1001 - val_out0_acc: 0.1010 - val_out1_acc: 0.8583 - val_out2_acc: 0.9762
accuracy:  0.8584
cost:      0.256059525839
Epoch 25/30
 - 62s - loss: 10.8808 - conf0_loss: 2.1666 - conf1_loss: 2.1666 - out0_loss: 2.1666 - out1_loss: 2.1666 - out2_loss: 2.1666 - conf0_acc: 0.9000 - conf1_acc: 0.1008 - out0_acc: 0.1022 - out1_acc: 0.8919 - out2_acc: 0.9817 - val_loss: 12.8038 - val_conf0_loss: 2.5512 - val_conf1_loss: 2.5512 - val_out0_loss: 2.5512 - val_out1_loss: 2.5512 - val_out2_loss: 2.5512 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1051 - val_out0_acc: 0.1010 - val_out1_acc: 0.7999 - val_out2_acc: 0.9806
accuracy:  0.8036
cost:      0.260746819555
Epoch 26/30
 - 61s - loss: 10.7848 - conf0_loss: 2.1473 - conf1_loss: 2.1473 - out0_loss: 2.1473 - out1_loss: 2.1473 - out2_loss: 2.1473 - conf0_acc: 0.9000 - conf1_acc: 0.1004 - out0_acc: 0.1023 - out1_acc: 0.8956 - out2_acc: 0.9818 - val_loss: 11.0073 - val_conf0_loss: 2.1917 - val_conf1_loss: 2.1917 - val_out0_loss: 2.1917 - val_out1_loss: 2.1917 - val_out2_loss: 2.1917 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1022 - val_out0_acc: 0.1045 - val_out1_acc: 0.8969 - val_out2_acc: 0.9736
accuracy:  0.8984
cost:      0.257993964515
Epoch 27/30
 - 61s - loss: 10.5604 - conf0_loss: 2.1023 - conf1_loss: 2.1023 - out0_loss: 2.1023 - out1_loss: 2.1023 - out2_loss: 2.1023 - conf0_acc: 0.9000 - conf1_acc: 0.1002 - out0_acc: 0.1028 - out1_acc: 0.9031 - out2_acc: 0.9855 - val_loss: 10.7435 - val_conf0_loss: 2.1390 - val_conf1_loss: 2.1390 - val_out0_loss: 2.1390 - val_out1_loss: 2.1390 - val_out2_loss: 2.1390 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1001 - val_out0_acc: 0.1014 - val_out1_acc: 0.8965 - val_out2_acc: 0.9836
accuracy:  0.8965
cost:      0.256059525839
Epoch 28/30
 - 62s - loss: 10.5418 - conf0_loss: 2.0986 - conf1_loss: 2.0986 - out0_loss: 2.0986 - out1_loss: 2.0986 - out2_loss: 2.0986 - conf0_acc: 0.9000 - conf1_acc: 0.1001 - out0_acc: 0.1045 - out1_acc: 0.9041 - out2_acc: 0.9849 - val_loss: 10.5920 - val_conf0_loss: 2.1087 - val_conf1_loss: 2.1087 - val_out0_loss: 2.1087 - val_out1_loss: 2.1087 - val_out2_loss: 2.1087 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1001 - val_out0_acc: 0.1061 - val_out1_acc: 0.9024 - val_out2_acc: 0.9820
accuracy:  0.9024
cost:      0.256059525839
Epoch 29/30
 - 62s - loss: 10.5242 - conf0_loss: 2.0951 - conf1_loss: 2.0951 - out0_loss: 2.0951 - out1_loss: 2.0951 - out2_loss: 2.0951 - conf0_acc: 0.9000 - conf1_acc: 0.1001 - out0_acc: 0.1063 - out1_acc: 0.9051 - out2_acc: 0.9845 - val_loss: 10.7963 - val_conf0_loss: 2.1495 - val_conf1_loss: 2.1495 - val_out0_loss: 2.1495 - val_out1_loss: 2.1495 - val_out2_loss: 2.1495 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1001 - val_out0_acc: 0.1140 - val_out1_acc: 0.9029 - val_out2_acc: 0.9800
accuracy:  0.9029
cost:      0.256059525839
Epoch 30/30
 - 61s - loss: 10.4957 - conf0_loss: 2.0894 - conf1_loss: 2.0894 - out0_loss: 2.0894 - out1_loss: 2.0894 - out2_loss: 2.0894 - conf0_acc: 0.9000 - conf1_acc: 0.1001 - out0_acc: 0.1134 - out1_acc: 0.9059 - out2_acc: 0.9855 - val_loss: 10.4102 - val_conf0_loss: 2.0723 - val_conf1_loss: 2.0723 - val_out0_loss: 2.0723 - val_out1_loss: 2.0723 - val_out2_loss: 2.0723 - val_conf0_acc: 0.9000 - val_conf1_acc: 0.1001 - val_out0_acc: 0.1165 - val_out1_acc: 0.9134 - val_out2_acc: 0.9843
accuracy:  0.9134
cost:      0.256059525839

T: 0.50 --> acc: 0.913400 cost: 0.256060
out0       1792 0.07%
out1       6608 0.26%
out2      25814 1.00%

Exit Distribution: <0, 9999, 1>
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 28, 28, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 28, 28, 4)    40          input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 28, 28, 4)    16          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 28, 28, 4)    0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 28, 28, 4)    148         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 28, 28, 4)    16          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 28, 28, 4)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 28, 4)    148         activation_2[0][0]               
__________________________________________________________________________________________________
add_1 (Add)                     (None, 28, 28, 4)    0           activation_1[0][0]               
                                                                 conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 28, 4)    16          add_1[0][0]                      
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 28, 4)    0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 14, 14, 8)    296         activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 14, 14, 8)    32          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 14, 14, 8)    0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 14, 14, 8)    40          add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 14, 14, 8)    584         activation_4[0][0]               
__________________________________________________________________________________________________
add_2 (Add)                     (None, 14, 14, 8)    0           conv2d_6[0][0]                   
                                                                 conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 14, 14, 8)    32          add_2[0][0]                      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 14, 14, 8)    0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 7, 7, 16)     1168        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 7, 7, 16)     64          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 7, 7, 16)     0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 7, 7, 16)     144         add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 7, 7, 16)     2320        activation_6[0][0]               
__________________________________________________________________________________________________
add_3 (Add)                     (None, 7, 7, 16)     0           conv2d_9[0][0]                   
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 7, 7, 16)     64          add_3[0][0]                      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 7, 7, 16)     0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 1, 1, 4)      0           add_1[0][0]                      
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 1, 1, 8)      0           add_2[0][0]                      
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 1, 1, 16)     0           activation_7[0][0]               
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 4)            0           average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 8)            0           average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 16)           0           average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
conf0 (Dense)                   (None, 1)            5           flatten_2[0][0]                  
__________________________________________________________________________________________________
conf1 (Dense)                   (None, 1)            9           flatten_3[0][0]                  
__________________________________________________________________________________________________
out0 (Dense)                    (None, 10)           50          flatten_2[0][0]                  
__________________________________________________________________________________________________
out1 (Dense)                    (None, 10)           90          flatten_3[0][0]                  
__________________________________________________________________________________________________
out2 (Dense)                    (None, 10)           170         flatten_1[0][0]                  
==================================================================================================
Total params: 5,452
Trainable params: 5,332
Non-trainable params: 120
__________________________________________________________________________________________________
